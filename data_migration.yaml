name: Historical Data Migration - Snowflake to BigQuery

on:
  push:
    branches:
      - main
  workflow_dispatch:

jobs:
  # Job for Cloud Composer DAG trigger
  trigger-composer-dag:
    runs-on: ubuntu-latest
    permissions:
      id-token: 'write'
      contents: 'read'

    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4

      - name: Google Authentication (Cloud Composer)
        id: auth
        uses: google-github-actions/auth@v2
        with:
          project_id: 'prj-composer-project-id'  # Use the correct project for Composer
          service_account: 'sa-composer@prj-composer-project-id.iam.gserviceaccount.com'
          workload_identity_provider: 'projects/463003052712/locations/global/workloadIdentityPools/wif-pool/providers/wif-provider'

      - name: Trigger Composer DAG
        run: |
          gcloud composer environments run us-east4-eda-composer-dev-1 \
          --location us-east4 dags trigger -- qaw_snowflake_to_bq_hist_payor

  # Job for Dataflow job execution
  run-dataflow-job:
    runs-on: ubuntu-latest
    permissions:
      id-token: 'write'
      contents: 'read'

    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4

      - name: Google Authentication (Dataflow)
        id: auth
        uses: google-github-actions/auth@v2
        with:
          project_id: 'prj-dataflow-project-id'  # Use the correct project for Dataflow
          service_account: 'sa-dataflow@prj-dataflow-project-id.iam.gserviceaccount.com'
          workload_identity_provider: 'projects/463003052712/locations/global/workloadIdentityPools/wif-pool/providers/wif-provider'

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.9"  # Adjust as needed

      - name: Install Dependencies
        run: |
          pip install google-cloud-dataflow  # Install required Dataflow package

      - name: Run Dataflow Job
        run: |
          python qaw_snowflake_to_bq_laborder_df_test.py \
            --runner=DataflowRunner \
            --job_name=qaw-snowflake-to-bq-hist-payor-test \
            --project=prj-eda-qadp-raw-dev-48699 \
            --region=us-east4 \
            --temp_location=gs://gcs-duse4usw1-eda-qadp-raw-dev-48699/dataflow/qaw-snowflake-to-bq-dataflow-job/temp \
            --staging_location=gs://gcs-duse4usw1-eda-qadp-raw-dev-48699/dataflow/qaw-snowflake-to-bq-dataflow-job/staging \
            --subnetwork=https://www.googleapis.com/compute/v1/projects/prj-shrd-ntwk-3/regions/us-east4/subnetworks/sub-non-prod-psa-ue4 \
            --no_use_public_ips \
            --input_data_location=gs://gcs-duse4usw1-eda-qadp-raw-dev-48699/source/snowflake/historical/lab_ordr_rslt_dgns_0_0_0.snappy.parquet \
            --labels=appserviceid=snsvc123456 \
            --labels=appservicename=google_cloud_platform \
            --labels=timestamp=resource_creation_time \
            --labels=iac=terraform \
            --labels=datatype=blank \
            --labels=tierid=tier-1 \
            --service_account_email=qadp-raw-dataflow-dev@prj-eda-qadp-raw-dev-48699.iam.gserviceaccount.com
